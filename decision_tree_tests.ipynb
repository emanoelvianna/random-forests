{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "9178afb9b3daeee402732af4e4aef10a7593df5184141cef26a75ebe4e69e442"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "dataset = pd.read_csv(\"C:/Users/renan/Desktop/UFRGS/Machine Learning/Trabalho1/random-forests-1/datasets/benchmark.csv\") \n",
    "\n",
    "dir(dataset)\n",
    "\n",
    "print ('Banco com %d amostras e %d colunas\\n' % dataset.shape)\n",
    "\n",
    "print ('Tabela sumarizando as colunas do banco\\n')\n",
    "dataset.describe()"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Banco com 14 amostras e 5 colunas\n\nTabela sumarizando as colunas do banco\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             Tempo Temperatura Umidade Ventoso Joga\n",
       "count           14          14      14      14   14\n",
       "unique           3           3       2       2    2\n",
       "top     Ensolarado       Amena    Alta   Falso  Sim\n",
       "freq             5           6       7       8    9"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tempo</th>\n      <th>Temperatura</th>\n      <th>Umidade</th>\n      <th>Ventoso</th>\n      <th>Joga</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>14</td>\n      <td>14</td>\n      <td>14</td>\n      <td>14</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Ensolarado</td>\n      <td>Amena</td>\n      <td>Alta</td>\n      <td>Falso</td>\n      <td>Sim</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n      <td>8</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(dataset):\n",
    "    bootstrap = np.random.choice(dataset, size=len(dataset), replace=True) \n",
    "    return bootstrap\n",
    "\n",
    "for data in dataset:\n",
    "    print(bootstrap_sample(data))\n",
    "\n",
    "#teste para o codigo com dados tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8aea83c72c67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    118\u001b[0m     ]\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_tree_id3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'----------------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-8aea83c72c67>\u001b[0m in \u001b[0;36mbuild_tree_id3\u001b[1;34m(inputs, split_candidates)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mbest_attribute\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcandidate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplit_candidates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mmean_entropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartition_entropy_by\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[INFO] Atributo: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'- entropia média: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_entropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mgain_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_entropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-8aea83c72c67>\u001b[0m in \u001b[0;36mpartition_entropy_by\u001b[1;34m(inputs, attribute)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpartition_entropy_by\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;34m\"\"\"computes the entropy corresponding to the given partition\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mpartitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartition_by\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpartition_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# retorna a media dos sub-conjuntos resultantes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-8aea83c72c67>\u001b[0m in \u001b[0;36mpartition_by\u001b[1;34m(inputs, attribute)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpartition_by\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;34m\"\"\"returns a dict of inputs partitioned by the attribute each input is a pair (attribute_dict, label)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgroup_by\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mattribute\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpartition_entropy_by\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-8aea83c72c67>\u001b[0m in \u001b[0;36mgroup_by\u001b[1;34m(items, key_fn)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgroup_by\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;34m\"\"\"returns a defaultdict(list), where each input item is in the list whose key is key_fn(item)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "def entropy(class_probabilities):\n",
    "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p * math.log(p, 2) for p in class_probabilities if p)\n",
    "\n",
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count\n",
    "            for count in Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labeled_data):\n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probabilities = class_probabilities(labels)\n",
    "    return entropy(probabilities)\n",
    "\n",
    "def partition_entropy(subsets):\n",
    "    \"\"\"find the entropy from this partition of data into subsets\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "\n",
    "    return sum( data_entropy(subset) * len(subset) / total_count for subset in subsets )\n",
    "\n",
    "def group_by(items, key_fn):\n",
    "    \"\"\"returns a defaultdict(list), where each input item is in the list whose key is key_fn(item)\"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for item in items:\n",
    "        key = key_fn(item)\n",
    "        groups[key].append(item)\n",
    "    return groups\n",
    "\n",
    "def partition_by(inputs, attribute):\n",
    "    \"\"\"returns a dict of inputs partitioned by the attribute each input is a pair (attribute_dict, label)\"\"\"\n",
    "    return group_by(inputs, lambda x: x[0][attribute])\n",
    "\n",
    "def partition_entropy_by(inputs, attribute):\n",
    "    \"\"\"computes the entropy corresponding to the given partition\"\"\"\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    return partition_entropy(partitions.values()) # retorna a media dos sub-conjuntos resultantes\n",
    "\n",
    "def gain(class_entropy, mean_entropy):\n",
    "    return class_entropy - mean_entropy;\n",
    "\n",
    "def class_entropy(inputs):\n",
    "    \"\"\"retorna a entropia sobre a classe\"\"\"\n",
    "    class_values = []\n",
    "    for item in inputs:\n",
    "        class_values.append(item[1]) # suponto a definição atual dos dados!\n",
    "    probabilities = class_probabilities(class_values)\n",
    "    class_entropy = entropy(probabilities)\n",
    "    return class_entropy\n",
    "\n",
    "def build_tree_id3(inputs, split_candidates=None):\n",
    "\n",
    "    # if this is our first pass,\n",
    "    # all keys of the first input are split candidates\n",
    "    if split_candidates is None:\n",
    "        split_candidates = inputs[0][0].keys()\n",
    "\n",
    "    # count Trues and Falses in the inputs\n",
    "    num_inputs = len(inputs)\n",
    "    num_trues = len([label for item, label in inputs if label])\n",
    "    num_falses = num_inputs - num_trues\n",
    "\n",
    "    if num_trues == 0:                  # if only Falses are left\n",
    "        return False                    # return a \"False\" leaf\n",
    "\n",
    "    if num_falses == 0:                 # if only Trues are left\n",
    "        return True                     # return a \"True\" leaf\n",
    "\n",
    "    if not split_candidates:            # if no split candidates left\n",
    "        return num_trues >= num_falses  # return the majority leaf\n",
    "\n",
    "    # otherwise, split on the best attribute\n",
    "    \n",
    "    best_gain = 0\n",
    "    best_attribute = None\n",
    "    for candidate in split_candidates:\n",
    "        mean_entropy = partition_entropy_by(inputs, candidate)\n",
    "        print('[INFO] Atributo: ', candidate, '- entropia média: ', mean_entropy)\n",
    "        gain_result = gain(class_entropy(inputs), mean_entropy)\n",
    "        print('[INFO] Atributo: ', candidate, '- ganho: ', gain_result)\n",
    "        if(gain_result > best_gain):\n",
    "            best_gain = gain(class_entropy(inputs), mean_entropy)\n",
    "            best_attribute = candidate\n",
    "        print('--')\n",
    "    \n",
    "    print('----------------------------------------')\n",
    "    print('[INFO] best_attribute: ', best_attribute)\n",
    "    print('----------------------------------------')\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_candidates = [a for a in split_candidates\n",
    "                      if a != best_attribute]\n",
    "\n",
    "    # recursively build the subtrees\n",
    "    subtrees = { attribute : build_tree_id3(subset, new_candidates)\n",
    "                 for attribute, subset in partitions.items() }\n",
    "\n",
    "    #subtrees[None] = num_trues > num_falses # default case\n",
    "\n",
    "    return (best_attribute, subtrees)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # inputs contendo tupulas contendo dicionarios\n",
    "    inputs = [\n",
    "        ({'Tempo':'Ensolarado','Temperatura':'Quente','Umidade':'Alta','Ventoso':'no'}, False),\n",
    "        ({'Tempo':'Ensolarado','Temperatura':'Quente','Umidade':'Alta','Ventoso':'yes'}, False),\n",
    "        ({'Tempo':'Nublado','Temperatura':'Quente','Umidade':'Alta','Ventoso':'no'}, True),\n",
    "        ({'Tempo':'Chuvoso','Temperatura':'Amena','Umidade':'Alta','Ventoso':'no'}, True),\n",
    "        ({'Tempo':'Chuvoso','Temperatura':'Fria','Umidade':'Normal','Ventoso':'no'}, True),\n",
    "        ({'Tempo':'Chuvoso','Temperatura':'Fria','Umidade':'Normal','Ventoso':'yes'}, False),\n",
    "        ({'Tempo':'Nublado','Temperatura':'Fria','Umidade':'Normal','Ventoso':'yes'}, True),\n",
    "        ({'Tempo':'Ensolarado','Temperatura':'Amena','Umidade':'Alta','Ventoso':'no'}, False),\n",
    "        ({'Tempo':'Ensolarado','Temperatura':'Fria','Umidade':'Normal','Ventoso':'no'}, True),\n",
    "        ({'Tempo':'Chuvoso','Temperatura':'Amena','Umidade':'Normal','Ventoso':'no'}, True),\n",
    "        ({'Tempo':'Ensolarado','Temperatura':'Amena','Umidade':'Normal','Ventoso':'yes'}, True),\n",
    "        ({'Tempo':'Nublado','Temperatura':'Amena','Umidade':'Alta','Ventoso':'yes'}, True),\n",
    "        ({'Tempo':'Nublado','Temperatura':'Quente','Umidade':'Normal','Ventoso':'no'}, True),\n",
    "        ({'Tempo':'Chuvoso','Temperatura':'Amena','Umidade':'Alta','Ventoso':'yes'}, False)\n",
    "    ]\n",
    "    \n",
    "    tree = build_tree_id3(inputs)\n",
    "    print('\\n')\n",
    "    print('----------------------------------------')\n",
    "    print(\"[INFO] Árvore construida:\")\n",
    "    print('----------------------------------------')\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9402859586706311\n"
     ]
    }
   ],
   "source": [
    "def entropia(dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculo da entropia do conjunto de dados.\n",
    "    Recebe como parâmetro o dataset. \n",
    "    \"\"\"\n",
    "\n",
    "    classe = dataset.keys()[-1]   #Define a variável alvo como classe\n",
    "    entropy = 0\n",
    "    values = dataset[classe].unique()\n",
    "    for value in values:\n",
    "        fraction = dataset[classe].value_counts()[value]/len(dataset[classe])\n",
    "        entropy += -fraction*np.log2(fraction)\n",
    "    return entropy\n",
    "\n",
    "#Teste dos valores gerados pela função \n",
    "teste_entropia = entropia(dataset)\n",
    "print (teste_entropia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Tempo': 0.6935361388961914, 'Temperatura': 0.9110633930116756, 'Umidade': 0.7884504573082889, 'Ventoso': 0.892158928262361}\n"
     ]
    }
   ],
   "source": [
    "eps = np.finfo(float).eps\n",
    "from numpy import log2 as log\n",
    "\n",
    "def entropia_atributos(dataset, attribute):\n",
    "    \n",
    "    classe = dataset.keys()[-1]   \n",
    "    target_variables = dataset[classe].unique()  \n",
    "    variables = dataset[attribute].unique()  \n",
    "    entropy_attribute = 0\n",
    "    \n",
    "    for variable in variables:\n",
    "        entropy_each_feature = 0\n",
    "        for target_variable in target_variables:\n",
    "            num = len(dataset[attribute][dataset[attribute] == variable][dataset[classe] == target_variable]) \n",
    "            den = len(dataset[attribute][dataset[attribute] == variable]) \n",
    "            fraction = num/(den + eps)  \n",
    "            entropy_each_feature += -fraction*log(fraction + eps) \n",
    "        fraction2 = den/len(dataset)\n",
    "        entropy_attribute += -fraction2*entropy_each_feature\n",
    "\n",
    "    return(abs(entropy_attribute))\n",
    "\n",
    "#Teste dos valores gerados pela função \n",
    "teste_entrobia_atributos = {k:entropia_atributos(dataset,k) for k in dataset.keys()[:-1]}\n",
    "print (teste_entrobia_atributos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'teste_entrobia_atributos' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-aceaa3eaa6c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#Teste dos valores gerados pela função\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mIG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mentropia_atributos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteste_entropia\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mteste_entrobia_atributos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mteste_entrobia_atributos\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'teste_entrobia_atributos' is not defined"
     ]
    }
   ],
   "source": [
    "def ganho_de_informacao(dataset):\n",
    "\n",
    "    IG = []\n",
    "\n",
    "    for key in dataset.keys()[:-1]:\n",
    "        IG.append(entropia(dataset)-entropia_atributos(dataset,key))\n",
    "    return dataset.keys()[:-1][np.argmax(IG)]\n",
    "\n",
    "#Teste dos valores gerados pela função\n",
    "IG = {k:entropia_atributos(teste_entropia,teste_entrobia_atributos[k]) for k in teste_entrobia_atributos}\n",
    "print (dataset.keys()[:-1][np.argmax(IG)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_particao(dataset, node, value):\n",
    "  return dataset[dataset[node] == value].reset_index(drop=True)\n",
    "  \n",
    "  \n",
    "def decision_tree(dataset, tree = None): \n",
    "    \n",
    "    classe = dataset.keys()[-1]\n",
    "    node = ganho_de_informacao(dataset)\n",
    "    attValue = np.unique(dataset[node])\n",
    "        \n",
    "    if tree is None:                    \n",
    "        tree={}\n",
    "        tree[node] = {} \n",
    "\n",
    "    for value in attValue:\n",
    "        \n",
    "        subtable = gerar_particao(dataset, node, value)\n",
    "        clValue,counts = np.unique(subtable['Eat'],return_counts=True)                        \n",
    "        \n",
    "        if len(counts)==1:\n",
    "            tree[node][value] = clValue[0]                                                    \n",
    "        else:        \n",
    "            tree[node][value] = decision_tree(subtable)\n",
    "                   \n",
    "    return tree\n",
    "\n",
    "teste = decision_tree(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}