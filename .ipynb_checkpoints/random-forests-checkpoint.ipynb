{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizado Ensemble - Florestas Aleatórias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bibliotecas utilizadas no desenvolvimento do projeto\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from functools import partial\n",
    "import math, random\n",
    "import pandas as pd\n",
    "from csv import reader\n",
    "import numpy as np\n",
    "from random import seed, randrange\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(path_file): \n",
    "    dataset = list()\n",
    "    with open(path_file, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        rows = [r for r in csv_reader]\n",
    "    # capturando os cabeçalhos\n",
    "    headers = rows[0]\n",
    "        \n",
    "    for row in rows[1:len(rows)]:\n",
    "        out_dic = {}\n",
    "        for header_index in range(len(headers)):\n",
    "            out_dic.update({ headers[header_index]: row[header_index]})\n",
    "        \n",
    "        # captura a classe\n",
    "        class_key = list(out_dic.items())[-1]\n",
    "        # remove a classe captura do dicionario\n",
    "        del out_dic[list(out_dic.keys())[-1]]\n",
    "        # montando o dataset de saída\n",
    "        data_tuple = (out_dic, class_key[1])\n",
    "        dataset.append(data_tuple)\n",
    "    return dataset    \n",
    "\n",
    "# construindo a base de dados a partir do arquivo fornecido\n",
    "dataset = build_dataset('datasets/benchmark.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dada uma lista de probabilidades de classe, calcule a entropia\n",
    "def entropy(class_probabilities):\n",
    "    return sum(-p * math.log(p, 2) for p in class_probabilities if p)\n",
    "\n",
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count\n",
    "            for count in Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labeled_data):\n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probabilities = class_probabilities(labels)\n",
    "    return entropy(probabilities)\n",
    "\n",
    "# encontre a entropia desta partição de dados em subconjuntos\n",
    "def partition_entropy(subsets):\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "\n",
    "    return sum( data_entropy(subset) * len(subset) / total_count for subset in subsets )\n",
    "\n",
    "# returns a defaultdict(list), where each input item is in the list whose key is key_fn(item)\n",
    "def group_by(items, key_fn):\n",
    "    groups = defaultdict(list)\n",
    "    for item in items:\n",
    "        key = key_fn(item)\n",
    "        groups[key].append(item)\n",
    "    return groups\n",
    "\n",
    "# retorna um dicionário de entradas particionado pelo atributo, cada entrada é um par (attribute_dict, label)\n",
    "def partition_by(inputs, attribute):\n",
    "    return group_by(inputs, lambda x: x[0][attribute])\n",
    "\n",
    "# calcula a entropia correspondente à partição dada\n",
    "def partition_entropy_by(inputs, attribute):\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    return partition_entropy(partitions.values()) # retorna a media dos sub-conjuntos resultantes\n",
    "\n",
    "def gain(class_entropy, mean_entropy):\n",
    "    return class_entropy - mean_entropy;\n",
    "\n",
    "# retorna a entropia sobre a classe\n",
    "def class_entropy(inputs):\n",
    "    class_values = []\n",
    "    for item in inputs:\n",
    "        class_values.append(item[1]) # suponto a definição atual dos dados!\n",
    "    probabilities = class_probabilities(class_values)\n",
    "    class_entropy = entropy(probabilities)\n",
    "    return class_entropy\n",
    "\n",
    "def diff_class(inputs):\n",
    "    classes = []\n",
    "    for current_class in list(set([label for item, label in inputs if label])):\n",
    "        classes.append({'class': current_class, 'amount': 0})\n",
    "    for current_class in [label for item, label in inputs if label]:\n",
    "        for my_class in classes:\n",
    "            if(my_class['class']  == current_class):\n",
    "                my_class['amount'] = my_class['amount'] + 1\n",
    "    return classes\n",
    "\n",
    "def highest_class(classes):\n",
    "    my_class = classes[0]\n",
    "    for current_class in classes:\n",
    "        if(current_class['amount'] > my_class['amount']):\n",
    "            my_class = current_class\n",
    "    return my_class\n",
    "    \n",
    "\n",
    "def build_tree(inputs, split_candidates=None):\n",
    "\n",
    "    # se esta for nossa primeira passagem,\n",
    "    # todas as chaves da primeira entrada são candidatas divididas\n",
    "    if split_candidates is None:\n",
    "        split_candidates = inputs[0][0].keys()\n",
    "              \n",
    "    # obtendo as classes\n",
    "    # [(classe, quantidade)]\n",
    "    classes = diff_class(inputs)\n",
    "    \n",
    "    # conte verdadeiras e falsas nas entradas\n",
    "    num_inputs = len(inputs)\n",
    "    num_trues = len([label for item, label in inputs if label])\n",
    "    num_falses = num_inputs - num_trues\n",
    "    \n",
    "    # verificando a homogeneidade do cojunto\n",
    "    if(len(classes) == 1):\n",
    "        return classes[0]['class']\n",
    "\n",
    "    if not split_candidates:          # if no split candidates left\n",
    "        return highest_class(classes) # return the majority leaf\n",
    "\n",
    "    # caso contrário, divida no melhor atributo\n",
    "    \n",
    "    best_gain = 0\n",
    "    best_attribute = None\n",
    "    for candidate in split_candidates:\n",
    "        mean_entropy = partition_entropy_by(inputs, candidate)\n",
    "        #print('[INFO] Atributo: ', candidate, '- entropia média: ', mean_entropy)\n",
    "        gain_result = gain(class_entropy(inputs), mean_entropy)\n",
    "        #print('[INFO] Atributo: ', candidate, '- ganho: ', gain_result)\n",
    "        if(gain_result > best_gain):\n",
    "            best_gain = gain(class_entropy(inputs), mean_entropy)\n",
    "            best_attribute = candidate\n",
    "        #print('--')\n",
    "    \n",
    "    #print('----------------------------------------')\n",
    "    #print('[INFO] best_attribute: ', best_attribute)\n",
    "    #print('----------------------------------------')\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_candidates = [a for a in split_candidates\n",
    "                      if a != best_attribute]\n",
    "\n",
    "    # construir recursivamente as subárvores\n",
    "    subtrees = { attribute : build_tree(subset, new_candidates)\n",
    "                 for attribute, subset in partitions.items() }\n",
    "\n",
    "    #subárvores [Nenhum] = num_trues> num_falses # caso padrão\n",
    "\n",
    "    return (best_attribute, subtrees)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # construindo a árvore sobre os dados construindos anteriormente\n",
    "    tree = build_tree(dataset)\n",
    "    #print('\\n')\n",
    "    #print('----------------------------------------')\n",
    "    #print(\"[INFO] Árvore construida:\")\n",
    "    #print('----------------------------------------')\n",
    "    #print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Digraph('G', filename='tree.gv')\n",
    "\n",
    "g.edge('Tempo, ganho 0.970', 'Umidade', label='Ensolarado, ganho: 0.970')\n",
    "g.edge('Umidade', 'Não', label='Alta')\n",
    "g.edge('Umidade', 'Sim', label='Normal')\n",
    "g.edge('Tempo, ganho 0.970', 'Sim', label='Nublado')\n",
    "g.edge('Tempo, ganho 0.970', 'Ventoso', label='Nublado, ganho: 0.970')\n",
    "g.edge('Ventoso', 'Sim', label='Falso')\n",
    "g.edge('Ventoso', 'Nao', label='Verdadeiro')\n",
    "\n",
    "#g.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INFO] Dados de amostras (ultimas duas linhas) para teste de resultado:\n",
      "Nublado / Quente / Normal / Falso -> Sim\n",
      "Chuvoso / Amena / Alta / Verdadeiro -> Nao\n"
     ]
    }
   ],
   "source": [
    "def classify(tree, classes, input):\n",
    "    \"\"\"classificar a entrada usando a árvore de decisão fornecida\"\"\"\n",
    "\n",
    "    # se este é um nó folha, retorna seu valor\n",
    "    if tree in classes:\n",
    "        return tree\n",
    "\n",
    "    # caso contrário, encontre a subárvore correta\n",
    "    attribute, subtree_dict = tree\n",
    "\n",
    "    subtree_key = input.get(attribute)  # Nenhum se a entrada estiver faltando atributo\n",
    "\n",
    "    if subtree_key not in subtree_dict: # se não houver subárvore para a chave\n",
    "        subtree_key = None              # vamos usar a subárvore não\n",
    "\n",
    "    subtree = subtree_dict[subtree_key] # escolha a subárvore apropriada\n",
    "    return classify(subtree, classes, input) # e usá-lo para classificar a entrada\n",
    "\n",
    "\n",
    "# obtendo as classes possiveis para a àrvore\n",
    "classes = list(set([label for item, label in dataset if label]))\n",
    "\n",
    "print('\\n')\n",
    "print('[INFO] Dados de amostras (ultimas duas linhas) para teste de resultado:')\n",
    "\n",
    "print('Nublado / Quente / Normal / Falso ->', classify(tree, classes,\n",
    "    { 'Tempo' : 'Nublado',\n",
    "      'Temperatura' : 'Quente',\n",
    "      'Umidade' : 'Normal',\n",
    "      'Ventoso' : 'Falso'} ))\n",
    "print('Chuvoso / Amena / Alta / Verdadeiro ->', classify(tree, classes,\n",
    "    { 'Tempo' : 'Chuvoso',\n",
    "      'Temperatura' : 'Amena',\n",
    "      'Umidade' : 'Alta',\n",
    "      'Ventoso' : 'Verdadeiro'} ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(dataset):\n",
    "    bootstrap = dataset.sample(n = len(dataset), replace = True)\n",
    "    return bootstrap\n",
    "\n",
    "result_bootstrap = bootstrap(pd.read_csv('datasets/voting.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "result_cross = cross_validation_split(result_bootstrap.values.tolist(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] definindo folds para teste\n",
      "\n",
      "[INFO] definindo folds para treino\n"
     ]
    }
   ],
   "source": [
    "def train_split(dataset, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        return train_set\n",
    "\n",
    "def test_split(dataset, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    for fold in folds:\n",
    "        test_set = list(fold)\n",
    "        return test_set\n",
    "\n",
    "\n",
    "# Não está retendo os dados do banco de bootstrap, é os mesmos dados, mas ainda tá flutuando e gerando listas diferentes, temos que discutir se isso tem algum problema \n",
    "\n",
    "\n",
    "test_set = test_split(result_cross, 10) \n",
    "print('[INFO] definindo folds para teste')\n",
    "\n",
    "print('\\n[INFO] definindo folds para treino')\n",
    "train_set = train_split(result_cross, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_from_split(headers, dataset_from_bootstrap): \n",
    "    new_dataset = list()    \n",
    "    for row in dataset_from_bootstrap[0:len(dataset_from_bootstrap)]:\n",
    "        out_dic = {}\n",
    "        for header_index in range(len(headers)):\n",
    "            out_dic.update({ headers[header_index]: row[header_index]})\n",
    "        \n",
    "        # captura a classe\n",
    "        class_key = list(out_dic.items())[-1]\n",
    "        # remove a classe captura do dicionario\n",
    "        del out_dic[list(out_dic.keys())[-1]]\n",
    "        # montando o dataset de saída\n",
    "        data_tuple = (out_dic, class_key[1])\n",
    "        new_dataset.append(data_tuple)\n",
    "    return new_dataset\n",
    "\n",
    "# obtebdo os cabeçalhos\n",
    "headers = result_bootstrap.columns.tolist()\n",
    "#print(build_tree(build_from_bootstrap(headers, result_bootstrap.values.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pre_dataset(old_dataset):\n",
    "    new_dataset = list()\n",
    "    for i in old_dataset:\n",
    "        for j in i:\n",
    "            new_dataset.append(j)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construindo a árvore a partir do split vindo do cross\n",
    "new_train = build_pre_dataset(train_set) # treino\n",
    "\n",
    "new_tree_from_train = build_tree(build_from_split(headers, new_train))\n",
    "\n",
    "# construindo a árvore a partir do split vindo do cross\n",
    "new_test = build_pre_dataset(test_set) # teste\n",
    "        \n",
    "classes = list(set([label for item, label in build_from_split(headers, new_train) if label]))\n",
    "     \n",
    "classe_teste = list()\n",
    "preditos_teste = list()\n",
    "for current in build_from_split(headers, new_test):\n",
    "    preditos_teste.append(current[1])\n",
    "    classe_teste.append(classify(new_tree_from_train, classes, current[0]))\n",
    "  \n",
    "#print('\\n[INFO] resultado real dos dados de teste: ')\n",
    "#print(classe_teste)\n",
    "#print('\\n[INFO] resultado preditos esperado: ')\n",
    "#print(preditos_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29  0]\n",
      " [ 0 14]]\n",
      "\n",
      "Média da acurácia: 1.00%\n",
      "Média da Recall: 1.00%\n",
      "Média da Precisão: 1.00%\n",
      "Média da f-score: 1.00%\n",
      "\n",
      "Desvio padrão da acurácia: 0.00%\n",
      "Desvio padrão da Recall: 0.00%\n",
      "Desvio padrão da Precisão: 0.00%\n",
      "Desvio padrão da f-score: 0.00%\n"
     ]
    }
   ],
   "source": [
    "matrix = list()\n",
    "accuracy = list()\n",
    "recall = list()\n",
    "precision = list()\n",
    "score = list()\n",
    "\n",
    "def classification_report(classe_teste, preditos_teste):\n",
    "    true_class = 'democrat'\n",
    "    negative_class = 'republican'\n",
    "    # declarando a frêquencia\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for (indice, v_real) in enumerate(classe_teste):\n",
    "        v_predito = preditos_teste[indice]\n",
    "        # se trata de um valor real da classe positiva\n",
    "        if v_real == true_class:\n",
    "            tp += 1 if v_predito == v_real else 0\n",
    "            fp += 1 if v_predito != v_real else 0\n",
    "        else:\n",
    "            tn += 1 if v_predito == v_real else 0\n",
    "            fn += 1 if v_predito != v_real else 0\n",
    "                \n",
    "    # calculando os valores de frêquencia\n",
    "    accuracy.append((tp+tn)/(tp+fp+tn+fn))\n",
    "    recall.append(tp/(tp+fn))\n",
    "    precision.append(tp/(tp+fp))\n",
    "    score.append(2*tp/(2*tp+fp+fn))\n",
    "    \n",
    "    # desenhando a matriz de confusão\n",
    "    print(np.array([\n",
    "        [ tp, fp ],\n",
    "        [ fn, tn ]\n",
    "    ]))\n",
    "    \n",
    "classification_report(classe_teste, preditos_teste)\n",
    "print('')\n",
    "print('Média da acurácia: %.2f%%' % (sum(accuracy)/float(len(accuracy))))\n",
    "print('Média da Recall: %.2f%%' % (sum(recall)/float(len(recall))))\n",
    "print('Média da Precisão: %.2f%%' % (sum(precision)/float(len(precision))))\n",
    "print('Média da f-score: %.2f%%' % (sum(score)/float(len(score))))\n",
    "print('')\n",
    "print(\"Desvio padrão da acurácia: {:.2f}%\".format(np.std(accuracy)*100))\n",
    "print(\"Desvio padrão da Recall: {:.2f}%\".format(np.std(recall)*100))\n",
    "print(\"Desvio padrão da Precisão: {:.2f}%\".format(np.std(precision)*100))\n",
    "print(\"Desvio padrão da f-score: {:.2f}%\".format(np.std(score)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_originais = pd.read_csv('datasets/voting.csv')\n",
    "\n",
    "n_trees = 15\n",
    "my_tress = list()\n",
    "my_cases_for_test = list()\n",
    "for n in range(n_trees):\n",
    "    n_bootstrap = bootstrap(dados_originais)       \n",
    "    n_cross = cross_validation_split(n_bootstrap.values.tolist(), 10)\n",
    "    my_cases_for_test.append(build_pre_dataset(test_split(n_cross, 10)))\n",
    "    train_set = train_split(n_cross, 10)\n",
    "    new_tree_from_train = build_tree(build_from_split(headers, new_train))\n",
    "    my_tress.append(new_tree_from_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_classify(trees, classes, input):\n",
    "    votes = [classify(tree, classes, input) for tree in trees]\n",
    "    vote_counts = Counter(votes)\n",
    "    return vote_counts.most_common(1)[0][0]\n",
    "\n",
    "classe_teste = list()\n",
    "preditos_teste = list()\n",
    "for current in build_from_split(headers, my_cases_for_test[0]):\n",
    "    preditos_teste.append(current[1])\n",
    "    classe_teste.append(forest_classify(my_tress, classes, current[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23  0]\n",
      " [ 1 19]]\n",
      "\n",
      "Média da acurácia: 0.99%\n",
      "Média da Recall: 0.98%\n",
      "Média da Precisão: 1.00%\n",
      "Média da f-score: 0.99%\n",
      "\n",
      "Desvio padrão da acurácia: 1.16%\n",
      "Desvio padrão da Recall: 2.08%\n",
      "Desvio padrão da Precisão: 0.00%\n",
      "Desvio padrão da f-score: 1.06%\n"
     ]
    }
   ],
   "source": [
    "classification_report(classe_teste, preditos_teste)\n",
    "print('')\n",
    "print('Média da acurácia: %.2f%%' % (sum(accuracy)/float(len(accuracy))))\n",
    "print('Média da Recall: %.2f%%' % (sum(recall)/float(len(recall))))\n",
    "print('Média da Precisão: %.2f%%' % (sum(precision)/float(len(precision))))\n",
    "print('Média da f-score: %.2f%%' % (sum(score)/float(len(score))))\n",
    "print('')\n",
    "print(\"Desvio padrão da acurácia: {:.2f}%\".format(np.std(accuracy)*100))\n",
    "print(\"Desvio padrão da Recall: {:.2f}%\".format(np.std(recall)*100))\n",
    "print(\"Desvio padrão da Precisão: {:.2f}%\".format(np.std(precision)*100))\n",
    "print(\"Desvio padrão da f-score: {:.2f}%\".format(np.std(score)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
