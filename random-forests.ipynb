{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizado Ensemble - Florestas Aleatórias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Desenvolvimento do algoritmo de indução de uma árvore de decisão, usando como critério de seleção de atributos para divisão de nós o **Ganho de Informação (baseado no conceito de entropia)**. **Tratando tanto atributos categóricos quanto numéricos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 0.]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Implementação considerando o algoritmo CART\n",
    "## O algoritmo CART induz tanto tanto árvores de classificação quanto árvores de regressão\n",
    "# Qual o número de árvores que será considerado (hiperparâmetros)? Devemos ter hiperparâmetros?\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def entropy_criterion(data, labels):\n",
    "  \"\"\" Entropy\n",
    "  Parameters\n",
    "  ----------\n",
    "  data: numpy array-like = [n_samples, n_features]\n",
    "  labels: numpy array-like, shape = [n_samples]\n",
    "  \n",
    "  Return\n",
    "  ------\n",
    "  entropy: float\n",
    "  \"\"\"\n",
    "  classes = np.unique(labels)\n",
    "  \n",
    "  s = 0\n",
    "  for c in classes:\n",
    "    p = np.mean(labels == c)\n",
    "    s -= p * np.log(p)\n",
    "    \n",
    "  return s\n",
    "  \n",
    "\n",
    "def gini_criterion(data, labels):\n",
    "  \"\"\" Gini Index\n",
    "  Parameters\n",
    "  ----------\n",
    "  data: numpy array-like = [n_samples, n_features]\n",
    "  labels: numpy array-like, shape = [n_samples]\n",
    "  \n",
    "  Return\n",
    "  ------\n",
    "  gini: float\n",
    "  \"\"\"\n",
    "  classes = np.unique(labels)\n",
    "  \n",
    "  s = 0\n",
    "  for c in classes:\n",
    "    p = np.mean(labels == c)\n",
    "    s += p * (1 - p)\n",
    "    \n",
    "  return s\n",
    "\n",
    "\n",
    "def find_cut_point(data, labels, impurity_criterion = gini_criterion):\n",
    "  \"\"\" find the best cut point \n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  data: numpy array-like = [n_samples, n_features]\n",
    "  labels: numpy array-like, shape = [n_samples]\n",
    "  impurity_criterion: callable, default=gini_criterion\n",
    "  \n",
    "  Return\n",
    "  ------\n",
    "  feature, threshold\n",
    "  \"\"\"\n",
    "  n_samples, n_features = data.shape\n",
    "\n",
    "  max_info_gain = np.iinfo(np.int32).min\n",
    "  feat_id = 0\n",
    "  best_threshold = 0\n",
    "\n",
    "  # pré-calculando a impureza da região atual\n",
    "  H_parent = impurity_criterion(data, labels)\n",
    "  # para cada um dos atributos\n",
    "  # vamos tentar encontrar o limiar que maximiza o ganho de informação\n",
    "  for j in range(n_features):\n",
    "    # só nos interessa os valores ordenados únicos \n",
    "    # do atributo j nessa região do espaço\n",
    "    values = np.unique(data[:, j])\n",
    "    \n",
    "    for i in range(values.shape[0] - 1):\n",
    "      # usamos o ponto médio dos valores possíveis\n",
    "      # como limiar candidato para o ponto de corte\n",
    "      threshold = (values[i] + values[i + 1]) / 2.\n",
    "\n",
    "      mask = data[:, j] <= threshold\n",
    "\n",
    "      info_gain = H_parent \\\n",
    "                  - (mask.sum() * impurity_criterion(data[mask], labels[mask]) \\\n",
    "                  + (~mask).sum() * impurity_criterion(data[~mask], labels[~mask])) \\\n",
    "                  / float(n_samples)\n",
    "\n",
    "      if max_info_gain < info_gain:\n",
    "        best_threshold = threshold\n",
    "        feat_id = j\n",
    "        max_info_gain = info_gain\n",
    "        \n",
    "  return feat_id, best_threshold\n",
    "\n",
    "\n",
    "def stopping_criterion(n_classes, depth, max_depth):\n",
    "  \"\"\" Stopping criterion\n",
    "  Parameters\n",
    "  ----------\n",
    "  n_classe: int\n",
    "            number of classes in the region, one means that the region is pure.\n",
    "  depth: int,\n",
    "          current tree depth\n",
    "  max_depth: int, default=None\n",
    "          maximal tree depth. None for fully grown tree.\n",
    "  Return\n",
    "  ------\n",
    "  bool\n",
    "  \"\"\"\n",
    "  return (max_depth is not None and max_depth == depth) or (n_classes == 1)\n",
    "\n",
    "def build_tree(data, labels, tree, depth = 1):\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    n_classes = classes.shape[0]\n",
    "\n",
    "    # critério de parada\n",
    "    if not stopping_criterion(n_classes, depth, tree.max_depth):\n",
    "        node = Node()\n",
    "\n",
    "        # encontra melhor ponto de corte dado a região atual do espaço\n",
    "        # de acordo com critério de impureza escolhido\n",
    "        feature, threshold = find_cut_point(data, labels, \n",
    "                                            tree.impurity_criterion)\n",
    "        \n",
    "        # aplicando o limiar para particionar o espaço\n",
    "        mask = data[:, feature] <= threshold\n",
    "        \n",
    "        # contruindo árvore recursivamente para\n",
    "        # os sub-espaço da direita e da esquerda.\n",
    "        left = build_tree(data[mask], labels[mask], tree, depth + 1)\n",
    "        right = build_tree(data[~mask], labels[~mask], tree, depth + 1)\n",
    "     \n",
    "        return Node(feature=feature, threshold=threshold, left=left, right=right)\n",
    "\n",
    "    # calcula a quantidade de exemplos por classe nesse nó folha\n",
    "    # e instancia um nó folha com essas quantidades, lembre-se que isso\n",
    "    # será usado para predição. \n",
    "    values = np.zeros(tree.n_classes)\n",
    "    values[classes] = counts\n",
    "    return Node(is_leaf=True, counts=values)\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "  \"\"\"Node\"\"\"\n",
    "  def __init__(self, feature=None, threshold=None,\n",
    "                     is_leaf=None, counts=None, left=None, right=None):\n",
    "    super(Node, self).__init__()\n",
    "    self.threshold = threshold\n",
    "    self.is_leaf = is_leaf\n",
    "    self.counts = counts\n",
    "    self.left = left\n",
    "    self.right = right\n",
    "    self.feature = feature\n",
    "    \n",
    "\n",
    "class DecisionTreeClassifier(object):\n",
    "  \"\"\"DecisionTreeClassifier\n",
    "  Parameters\n",
    "  ----------\n",
    "  max_depth:\n",
    "  impurity_criterion:\n",
    "  \"\"\"\n",
    "  def __init__(self, max_depth, impurity_criterion = gini_criterion):\n",
    "    super(DecisionTreeClassifier, self).__init__()\n",
    "    self.max_depth = max_depth\n",
    "    self.impurity_criterion = impurity_criterion\n",
    "\n",
    "  def recursive_predict(self, node, X):\n",
    "\n",
    "    if node.is_leaf:\n",
    "      return np.zeros(X.shape[0]) + np.argmax(node.counts)\n",
    "\n",
    "    mask = X[:, node.feature] <= node.threshold\n",
    "\n",
    "    y_pred = np.zeros(X.shape[0])\n",
    "    if mask.sum() > 0:\n",
    "      y_pred[mask] = self.recursive_predict(node.left, X[mask])\n",
    "\n",
    "    if (~mask).sum() > 0:\n",
    "      y_pred[~mask] = self.recursive_predict(node.right, X[~mask])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    self.classes = np.unique(y)\n",
    "    self.n_classes = self.classes.shape[0]\n",
    "\n",
    "    self.root = build_tree(X, y, self)\n",
    "\n",
    "    return self\n",
    "\n",
    "  def predict(self, X):\n",
    "    return self.recursive_predict(self.root, X)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "if __name__ == '__main__':\n",
    "  X = np.array([[1,1], [1,0], [0,1], [0,0]])\n",
    "  y = np.array([0, 1, 1, 0])\n",
    "\n",
    "  dt = DecisionTreeClassifier(max_depth=None, impurity_criterion = entropy_criterion)\n",
    "\n",
    "  print(dt.fit(X, y).predict(X))\n",
    "  \n",
    "  X, y = load_iris(return_X_y=True)\n",
    "\n",
    "  y_pred = dt.fit(X, y).predict(X)\n",
    "\n",
    "  print(np.mean(y_pred == y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referências\n",
    "\n",
    "1. [Aprendendo em uma Floresta Aleatória](https://medium.com/machina-sapiens/o-algoritmo-da-floresta-aleat%C3%B3ria-3545f6babdf8)\n",
    "2. [Otimizando os hiperparâmetros](https://medium.com/data-hackers/otimizando-os-hiperpar%C3%A2metros-621de5e9be37)\n",
    "3. [Classificadores Ensemble](https://lamfo-unb.github.io/2017/09/27/BaggingVsBoosting/)\n",
    "4. [Bootstrap](https://lamfo-unb.github.io/2017/06/28/Bootstrap/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
